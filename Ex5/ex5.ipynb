{"cells":[{"cell_type":"markdown","id":"4e48f535","metadata":{"id":"4e48f535"},"source":["**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution.**"]},{"cell_type":"markdown","id":"90f7199d-c442-4732-ab89-83bda42682a7","metadata":{"id":"90f7199d-c442-4732-ab89-83bda42682a7"},"source":["# Exercise 5: Machine Translation with Transformers\n","\n","In this exercise, you will explore the Transformer-based neural architecture by tackling a machine translation task. Machine translation involves translating text from a source language to a target language. Traditionally, machine translation was performed using recurrent neural network (RNN)-based architectures. However, the emergence of Transformers has marked a revolutionary shift in the fields of text analysis and especially machine translation.\n","\n","Transformers offer several advantages over earlier RNN-based models for the machine translation task:\n","\n","* One major benefit is their ability to capture context from a long sequence using self-attention layers, which allows the model to retain relevant information from words further back in the text.\n","  \n","* Additionally, Transformers improve the natural flow and grammatical accuracy of translated sentences. Unlike RNN models, which tend to follow the word order of the source language, Transformers utilize cross-attention layers between the source and target languages. This allows them to arrange translated words in an order that sounds more natural in the target language, even if it differs significantly from the source structure.\n","  \n","* Finally, Transformers enable parallelization, which makes it feasible to train them on multiple GPUs, speeding up the training process.\n","\n","\n","To complete this assignment, you will progress through four different stages (tasks):\n","\n","**Task 1. Data Preparation (5 points)**\n","\n","**Task 2. Model Architecture (5 points)**\n","\n","**Task 3. Training and Evaluation (5 points)**\n","\n","**Task 4. Autoregressive Translation (5 points)**\n","\n","### **Deliverables:**\n","\n","Please submit below files to Moodle:\n","\n","* ex5.ipynb\n","* 'model.pth'\n","* 'translation.npy'\n","\n","### **Data**\n","\n","The dataset used for this exercise consists of a set of French sentenceas and their equivalent English translations.\n","\n","*Note:* Your dataset path should point to the \"dataset_ex5\" folder, which contains two CSV files, each containing 137860 short sentences:\n","\n","    small_vocab_fr.csv: French sentences.\n","    small_vocab_en.csv: Corresponding English translations.\n","\n","Be mindful of any extra folder levels that may be created when extracting the \"dataset_ex5.zip\" file.\n","\n","### **Useful links**\n","\n","* https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n","* https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","\n","\n","After downloading the data and setting up the folders, you are ready to begin the exercise tasks. Let's get started!\n"]},{"cell_type":"code","execution_count":23,"id":"b6f8445f","metadata":{"id":"b6f8445f","executionInfo":{"status":"ok","timestamp":1733009630000,"user_tz":-120,"elapsed":1052,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["skip_training = False   # You can set it to True if you want to run inference on your trained model."]},{"cell_type":"code","execution_count":24,"id":"2644c92a-195e-42a9-b63b-0f58e1f13518","metadata":{"id":"2644c92a-195e-42a9-b63b-0f58e1f13518","executionInfo":{"status":"ok","timestamp":1733009630000,"user_tz":-120,"elapsed":3,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["# Do not delete this cell"]},{"cell_type":"markdown","id":"b4929157-7d8f-4e84-85f6-c56ea7093ae1","metadata":{"id":"b4929157-7d8f-4e84-85f6-c56ea7093ae1"},"source":["Add path to the folder containing csv files."]},{"cell_type":"code","execution_count":25,"id":"959359d2-e60b-4dec-a5eb-d7dabe3f1a5f","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733009630000,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"},"user_tz":-120},"id":"959359d2-e60b-4dec-a5eb-d7dabe3f1a5f"},"outputs":[],"source":["path = \"dataset_ex5\" # you can change the path if you want to store the dataset somewhere else."]},{"cell_type":"code","execution_count":26,"id":"905cfe39-11bc-4c1a-9b6b-1d392a7bbd2c","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733009630000,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"},"user_tz":-120},"id":"905cfe39-11bc-4c1a-9b6b-1d392a7bbd2c"},"outputs":[],"source":["# Do not delete this cell"]},{"cell_type":"markdown","id":"cbdb6f08","metadata":{"id":"cbdb6f08"},"source":["Import all necessary libraries."]},{"cell_type":"code","execution_count":27,"id":"0b6feccd","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733009630001,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"},"user_tz":-120},"id":"0b6feccd"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","import os\n","from collections import Counter\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import random_split\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Set random seeds for all libraries\n","import random\n","random.seed(1)\n","np.random.seed(1)\n","torch.manual_seed(1)\n","torch.cuda.manual_seed(1)\n","torch.cuda.manual_seed_all(1)\n","\n","# Ensure deterministic behavior\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","id":"607dc4e5","metadata":{"id":"607dc4e5"},"source":["Select the device"]},{"cell_type":"code","execution_count":28,"id":"1a9f32a0","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1733009630990,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"},"user_tz":-120},"id":"1a9f32a0"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","id":"219ed6c0-92b9-4ca5-8b9f-71995799314f","metadata":{"id":"219ed6c0-92b9-4ca5-8b9f-71995799314f"},"source":["## Task 1: Data Preparation (5 Points)\n","\n","In this task, you will preprocess the dataset to convert it into a format suitable for input to a Transformer neural model. Each subtask focuses on a specific step in the preprocessing pipeline.\n","\n","### Summary of Tasks for This Stage\n","\n","**Task 1.1: Tokenizaion** (1 point)\n","\n","**Task 1.2: Building Vocabulary** (1 point)\n","\n","**Task 1.3: Sentence Embedding** (1 points)\n","\n","**Task 1.4: Positional Encodding** (2 points)\n"]},{"cell_type":"markdown","id":"b9febf38-0a1a-4ff3-94a7-eb232c21eef1","metadata":{"id":"b9febf38-0a1a-4ff3-94a7-eb232c21eef1"},"source":["### Task 1.1: Tokenizaion\n","\n","In this task, we use a basic tokenization method for simplicity and to avoid potential library mismatch issues that could arise across different students' systems and environments. While more advanced tokenization methods are available through specialized libraries, this basic approach ensures consistency and focuses on the core concept of tokenization. Our method uses Python's built-in tools and includes the following steps:\n","\n","1. Lowercasing: Convert all characters in the sentence to lowercase.\n","2. Filtering Characters: Define a set of characters to be removed from the sentences, replacing them with an empty string.\n","3. Splitting: Split the sentence into tokens based on spaces.\n","   \n","Run the cell below to load the data, observe basic statistics, and examine some sample sentences."]},{"cell_type":"code","execution_count":29,"id":"04ecf7ac-b2ce-4e9e-9560-8bc53c616282","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733009634035,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"},"user_tz":-120},"id":"04ecf7ac-b2ce-4e9e-9560-8bc53c616282","outputId":"2389d9a3-03c8-4934-8da0-83fb13d5b4e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 137860 English sentences in data\n","There are 137860 French sentences in data\n","Here are some examples:\n","----------\n","new jersey is sometimes quiet during autumn \n","new jersey est parfois calme pendant l' automne \n","----------\n","they like strawberries \n","ils aiment les fraises \n","----------\n","she plans to visit the united states next may .\n","elle envisage de se rendre aux états-unis en mai prochain .\n","____________________________________________________________________________________________________\n"]}],"source":["# Load your data\n","en_df = pd.read_csv(os.path.join(path , 'small_vocab_en.csv'), header=None, usecols=[0])\n","fr_df = pd.read_csv(os.path.join(path, 'small_vocab_fr.csv'), header=None, usecols=[0])\n","\n","english_sentences = en_df[0].values\n","french_sentences = fr_df[0].values\n","\n","print(f'There are {len(english_sentences)} English sentences in data')\n","print(f'There are {len(french_sentences)} French sentences in data')\n","print('Here are some examples:')\n","e = [ 0, 1000, 3000]\n","for i in e:\n","    print(10*\"-\")\n","    print(english_sentences[i])\n","    print(french_sentences[i])\n","print(100*\"_\")"]},{"cell_type":"markdown","id":"b30eefb6-5483-4cf5-839c-eb72aa892eb8","metadata":{"id":"b30eefb6-5483-4cf5-839c-eb72aa892eb8"},"source":["Complete the \"tokenize\" function by filling in the blanks based on the detailed guidance provided within the code comments."]},{"cell_type":"code","execution_count":30,"id":"05a18098-1a7f-4758-84f4-4a7f6bfa40d3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1716,"status":"ok","timestamp":1733009636694,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"},"user_tz":-120},"id":"05a18098-1a7f-4758-84f4-4a7f6bfa40d3","outputId":"da713553-a08f-42ca-a6ad-5a849abd59b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------\n","['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn']\n","['new', 'jersey', 'est', 'parfois', 'calme', 'pendant', \"l'\", 'automne']\n","----------\n","['they', 'like', 'strawberries']\n","['ils', 'aiment', 'les', 'fraises']\n","----------\n","['she', 'plans', 'to', 'visit', 'the', 'united', 'states', 'next', 'may']\n","['elle', 'envisage', 'de', 'se', 'rendre', 'aux', 'états', 'unis', 'en', 'mai', 'prochain']\n"]}],"source":["# Tokenize function\n","def tokenize(sentences):\n","    \"\"\"\n","    Tokenizes a list of sentences by:\n","    1. Converting all text to lowercase.\n","    2. Removing special characters listed in \"filters\".\n","    Hint: you can use \"str.maketrans\" to creates a translation table to remove unwanted characters defined in \"filters\".\n","    3. Splitting each sentence into a list of words.\n","    \"\"\"\n","    filters = '.?!#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n'\n","\n","    # YOUR CODE HERE\n","    sentences = [sentence.lower() for sentence in sentences]\n","    filtered_sentences = [sentence.translate(str.maketrans(filters, ' '*len(filters))) for sentence in sentences]\n","    tokenized_list = [sentence.split() for sentence in filtered_sentences]\n","\n","    return tokenized_list\n","\n","# Tokenize English and French sentences\n","tokenized_en = tokenize(english_sentences)\n","tokenized_fr = tokenize(french_sentences)\n","for i in e:\n","    print(10*\"-\")\n","    print(tokenized_en[i])\n","    print(tokenized_fr[i])"]},{"cell_type":"markdown","id":"b5aec479-83e4-447c-8a1d-3fdb7345e560","metadata":{"id":"b5aec479-83e4-447c-8a1d-3fdb7345e560"},"source":["Run the cell below to check the correctness of your solution to the tokenize function."]},{"cell_type":"code","execution_count":31,"id":"aa45a918-bf00-4e55-b019-931c192d0d31","metadata":{"id":"aa45a918-bf00-4e55-b019-931c192d0d31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009636694,"user_tz":-120,"elapsed":2,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"66742350-329f-43ca-b5d3-129ecdc38e29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test passed: The tokenize function is working as expected.\n"]}],"source":["# Visible tests here\n","\n","# Test the tokenize function with example sentences\n","test_sentences = [\"Hello, world!\", \"Python is fun.\", \"Let's tokenize this: right?\"]\n","\n","# Expected output: lowercase, special characters removed, tokenized words\n","expected_output = [\n","    [\"hello\", \"world\"],\n","    [\"python\", \"is\", \"fun\"],\n","    [\"let's\", \"tokenize\", \"this\", \"right\"]\n","]\n","\n","# Run the student's tokenize function\n","tokenized_output = tokenize(test_sentences)\n","\n","# Check if the output matches the expected output\n","assert tokenized_output == expected_output, (\n","    f\"Test failed!\\nExpected: {expected_output}\\nGot: {tokenized_output}\")\n","print(\"Test passed: The tokenize function is working as expected.\")\n"]},{"cell_type":"markdown","id":"bfc71a52-0148-4bce-af72-799047b0f653","metadata":{"id":"bfc71a52-0148-4bce-af72-799047b0f653"},"source":["### Task 1.2: Building Vocabulary\n","\n","In this step, we will convert tokenized sentences into lists of integers. This is achieved by defining a dictionary of unique words for each language and assigning a unique integer to each word.\n","\n","You may recall practicing a similar concept in Exercise 4, where you built a character-based dictionary. In this exercise, we are building a word-level dictionary, where each entry in the dictionary represents a unique word.\n","\n","In addition to the set of unique words in the dataset, the vocabulary must include three special tokens:\n","\n","1.  PAD: Padding Token (0)\n","2.  SOS: Start of Sentence (1)\n","3.  EOS: End of Sentence (2)\n","\n","Complete the \"build_vocab\" function by filling in the blanks according to the provided instructions in the code."]},{"cell_type":"code","execution_count":32,"id":"ee1d488c-b866-46da-91c9-857bc7540d7b","metadata":{"id":"ee1d488c-b866-46da-91c9-857bc7540d7b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009639341,"user_tz":-120,"elapsed":5,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"ed1fb7b4-2dda-417d-de53-a8b89f3e5d4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here are some examples from our English dictionary: \n","----------------------------------------------------------------------------------------------------\n","word: <PAD>, index: 0\n","word: <SOS>, index: 1\n","word: <EOS>, index: 2\n","word: new, index: 3\n","word: jersey, index: 4\n","word: is, index: 5\n","word: sometimes, index: 6\n","word: quiet, index: 7\n","word: during, index: 8\n","word: autumn, index: 9\n","__________\n","index: 0, word: <PAD>\n","index: 1, word: <SOS>\n","index: 2, word: <EOS>\n","index: 3, word: new\n","index: 4, word: jersey\n","index: 5, word: is\n","index: 6, word: sometimes\n","index: 7, word: quiet\n","index: 8, word: during\n","index: 9, word: autumn\n"]}],"source":["# Create vocabulary with special tokens\n","def build_vocab(tokenized_sentences):\n","    special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n","    # build vocab by applying \"Counter\" for sentence in tokenized_sentences and for token in sentence\n","    # add special tokens\n","    # word2idx = ? (a dictionary for mapping word to index)\n","    # idx2word = ? (a dictionary for index to word)\n","\n","    # YOUR CODE HERE\n","    word_counter = Counter(special_tokens)\n","    word_counter.update(token for sentence in tokenized_sentences for token in sentence)\n","    word2idx = {word: idx for idx, word in enumerate(word_counter)}\n","    idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","    return word2idx, idx2word\n","\n","en_word2idx, en_idx2word = build_vocab(tokenized_en)\n","fr_word2idx, fr_idx2word = build_vocab(tokenized_fr)\n","\n","print(\"Here are some examples from our English dictionary: \")\n","print(100 * \"-\")\n","\n","# Display first 10 words and their indices from en_word2idx\n","for i, (key, value) in enumerate(en_word2idx.items()):\n","    print(f'word: {key}, index: {value}')\n","    if i == 9:  # After 10 iterations, break\n","        break\n","\n","print(10 * \"_\")\n","\n","# Display first 10 indices and their words from en_idx2word\n","for i, (key, value) in enumerate(en_idx2word.items()):\n","    print(f'index: {key}, word: {value}')\n","    if i == 9:  # After 10 iterations, break\n","        break"]},{"cell_type":"markdown","id":"275fc4ac-53c1-4bf7-bd42-a7d601723865","metadata":{"id":"275fc4ac-53c1-4bf7-bd42-a7d601723865"},"source":["Run the cell below to check the correctness of your solution to the build_vocab function."]},{"cell_type":"code","execution_count":33,"id":"f547dd82-5bb6-47fb-b0dc-b0800fb904b3","metadata":{"id":"f547dd82-5bb6-47fb-b0dc-b0800fb904b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009641701,"user_tz":-120,"elapsed":2,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"778ab0b1-7bbd-46aa-c938-b81de6155f07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary test passed!\n"]}],"source":["# Visible tests here\n","\n","# Test: Check if the vocabulary is built correctly\n","\n","# Tokenized test data\n","sample_tokenized_sentences = [[\"hello\", \"world\"], [\"hello\", \"my\", \"friend\"]]\n","\n","# Expected results\n","expected_special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n","expected_vocab = expected_special_tokens + [\"friend\", \"hello\", \"my\", \"world\"]\n","\n","# Build vocabulary\n","word2idx, idx2word = build_vocab(sample_tokenized_sentences)\n","\n","# Test special tokens are present and in correct order\n","assert all(token in word2idx for token in expected_special_tokens), \"Special tokens missing from vocabulary\"\n","assert word2idx[\"<PAD>\"] == 0, \"<PAD> token index is incorrect\"\n","assert word2idx[\"<SOS>\"] == 1, \"<SOS> token index is incorrect\"\n","assert word2idx[\"<EOS>\"] == 2, \"<EOS> token index is incorrect\"\n","\n","# Test all unique words are present and sorted correctly\n","assert sorted(word2idx.keys()) == sorted(expected_vocab), \"Vocabulary does not match expected words\"\n","assert len(word2idx) == len(idx2word), \"Mismatch between word2idx and idx2word lengths\"\n","assert all(idx2word[word2idx[word]] == word for word in word2idx), \"word2idx and idx2word mappings are incorrect\"\n","\n","print(\"Vocabulary test passed!\")\n"]},{"cell_type":"markdown","id":"4604ca38-063e-46cb-a40e-57192653a316","metadata":{"id":"4604ca38-063e-46cb-a40e-57192653a316"},"source":["### Dataset Class\n","\n","In this step, we will use a custom dataset class specifically designed for our translation task. You do not need to implement anything for this step, as the dataset class is already provided for you. This class incorporates essential preprocessing steps, such as padding, truncation, and the addition of special tokens.\n","\n","As discussed earlier, sentences in our dataset have varying lengths. To ensure all sentences are of the same length (a requirement for the Transformer model), we will define a fixed length for input sequences. The preprocessing steps performed by the dataset class include:\n","\n","* Special Tokens: The 'SOS' token is added at the beginning of each sentence, and the 'EOS' token is added at the end.\n","* Truncation: Sentences that exceed the defined maximum length will be truncated to fit the specified length.\n","* Padding: The 'PAD' token is appended to sentences shorter than the maximum length until they reach the required length.\n","\n","Take some time to go through the dataset class and its methods to observe how these preprocessing steps are implemented. Understanding the class structure will help you in tasks where you might need to customize or extend the dataset functionality.\n"]},{"cell_type":"code","execution_count":34,"id":"742ff0d6-758b-43c6-a870-aab4a73746dd","metadata":{"id":"742ff0d6-758b-43c6-a870-aab4a73746dd","executionInfo":{"status":"ok","timestamp":1733009644104,"user_tz":-120,"elapsed":3,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"bd8c3c88-dc4c-4439-8cea-3f4497548c88","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Source batch: tensor([[  1,  40,  66,   5,   6, 105,  16,  65,   2,   0]], device='cuda:0')\n","torch.Size([1, 10])\n","__________\n","Target batch: tensor([[ 1, 58,  5,  6, 87,  8, 57,  2,  0,  0]], device='cuda:0')\n","torch.Size([1, 10])\n","__________\n"]}],"source":["# Dataset class with padding applied in __getitem__\n","class TranslationDataset(Dataset):\n","    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, seq_len=30):\n","        self.src_sentences = src_sentences\n","        self.tgt_sentences = tgt_sentences\n","        self.src_vocab = src_vocab\n","        self.tgt_vocab = tgt_vocab\n","        self.seq_len = seq_len\n","\n","    def __len__(self):\n","        return len(self.src_sentences)\n","\n","    def pad_sequence(self, tokens, vocab, is_target=False):\n","        \"\"\"\n","        Pads a sequence of tokens to the fixed length `seq_len`.\n","        Adds <SOS> at the start, <EOS> at the end, and pads with <PAD>.\n","        Trims if the sequence is longer than `seq_len`.\n","        \"\"\"\n","        tokens = [vocab[\"<SOS>\"]] + [vocab.get(token, vocab[\"<PAD>\"]) for token in tokens]\n","        tokens.append(vocab[\"<EOS>\"])\n","        tokens = tokens[:self.seq_len]\n","        tokens += [vocab[\"<PAD>\"]] * (self.seq_len - len(tokens))\n","        return tokens\n","\n","    def __getitem__(self, idx):\n","\n","        src_tokens = self.src_sentences[idx]\n","        tgt_tokens = self.tgt_sentences[idx]\n","\n","        # Apply padding to both the source and target sentences\n","        src_padded = self.pad_sequence(src_tokens, self.src_vocab, is_target=False)\n","        tgt_padded = self.pad_sequence(tgt_tokens, self.tgt_vocab, is_target=True)\n","\n","        # Convert to tensors and move to device (GPU or CPU)\n","        src_item = torch.tensor(src_padded).to(device)\n","        tgt_item = torch.tensor(tgt_padded).to(device)\n","\n","        return src_item, tgt_item\n","\n","# Instantiate and test the dataset, let the French be as source language and English as target language.\n","dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# Test the DataLoader\n","for src_batch, tgt_batch in dataloader:\n","    print(\"Source batch:\", src_batch)\n","    print(src_batch.size())\n","    print(10*\"_\")\n","    print(\"Target batch:\", tgt_batch)\n","    print(tgt_batch.size())\n","    print(10*\"_\")\n","    break"]},{"cell_type":"markdown","id":"c9cbf6fe-050c-4357-ae8f-112a4021b231","metadata":{"id":"c9cbf6fe-050c-4357-ae8f-112a4021b231"},"source":["### Task 1.3: Sentence Embedding\n","\n","Words, by themselves, are discrete symbols that neural networks cannot process directly. To make them understandable to the model, we use **embedding layers**. These layers transform words or tokens into dense, fixed-size vectors, where each word is represented by a unique vector. The embedding layer maps words into a continuous vector space, enabling semantically similar words to be closer to each other in this space. This approach is far more compact and efficient than sparse, high-dimensional representations like one-hot encoding.\n","\n","Embedding layers are particularly useful in natural language processing tasks such as **machine translation**, where words in different languages must be represented in a way that enables the model to learn their relationships.\n","\n","In this step, you are asked to define separate PyTorch embedding layers for both the source and target languages, and then pass the src_batch and tgt_batch through these layers. In the cell below, complete the code by filling in the blanks according to the provided instructions. After running the cell, pay attention to the input and output dimensions of the embedding layers.\n"]},{"cell_type":"code","execution_count":35,"id":"9fe7f794-94bc-4332-a186-7c0d11fc218f","metadata":{"id":"9fe7f794-94bc-4332-a186-7c0d11fc218f","executionInfo":{"status":"ok","timestamp":1733009646162,"user_tz":-120,"elapsed":6,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"d49f7b3d-091b-43b1-91e9-ff317404b2e8","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["__________\n","torch.Size([1, 10])\n","torch.Size([1, 10, 128])\n","__________\n","torch.Size([1, 10])\n","torch.Size([1, 10, 128])\n"]}],"source":["embedding_size = 128\n","vsize_src = len(fr_word2idx)\n","vsize_tgt = len(en_word2idx)\n","\n","# data: let the French be as source language and English as target language.\n","dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# Use 'next' to get a batch from the DataLoader iterator\n","src_batch, tgt_batch = next(iter(dataloader))\n","\n","# embedding_fr = ? (define an embedding layer for French words in your French Vocabulary)\n","# embedding_fr.to(device)\n","# output_embedding_fr = ? (pass src_batch through embedding_fr)\n","\n","# embedding_en = ? (define an embedding layer for English words in your English Vocabulary)\n","# embedding_en.to(device)\n","# output_embedding_en = ?\n","\n","# YOUR CODE HERE\n","embedding_fr = nn.Embedding(vsize_src, embedding_size)\n","embedding_fr.to(device)\n","output_embedding_fr = embedding_fr(src_batch)\n","\n","embedding_en = nn.Embedding(vsize_tgt, embedding_size)\n","embedding_en.to(device)\n","output_embedding_en = embedding_en(tgt_batch)\n","\n","print(10*\"_\")\n","print(src_batch.size())\n","print(output_embedding_fr.size())\n","print(10*\"_\")\n","print(tgt_batch.size())\n","print(output_embedding_en.size())"]},{"cell_type":"markdown","id":"0bc911b0-97c1-4924-817f-417696c65d44","metadata":{"id":"0bc911b0-97c1-4924-817f-417696c65d44"},"source":["Run the cell below to check the correctness of your solution for the sentence embedding."]},{"cell_type":"code","execution_count":36,"id":"30669a20-b2f9-4e86-96ac-15af4cdb01b9","metadata":{"id":"30669a20-b2f9-4e86-96ac-15af4cdb01b9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009648250,"user_tz":-120,"elapsed":5,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"b89d453a-465f-4e35-ed46-55b17f569e9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings are correctly implemented!\n"]}],"source":["# Visible tests here\n","\n","# Check if the embeddings have the correct output shapes\n","assert output_embedding_fr.shape == (src_batch.size(0), src_batch.size(1), embedding_size), \"Embedding for French is incorrect!\"\n","assert output_embedding_en.shape == (tgt_batch.size(0), tgt_batch.size(1), embedding_size), \"Embedding for English is incorrect!\"\n","\n","print(\"Embeddings are correctly implemented!\")"]},{"cell_type":"markdown","id":"1ce8ae0f-ece1-4596-8037-89631b8d2075","metadata":{"id":"1ce8ae0f-ece1-4596-8037-89631b8d2075"},"source":["### Task 1.4: Positional encoding\n","\n","In sequence models like the Transformer, the model needs a way to understand the relative positions of words in a sequence. Since the Transformer model does not inherently process sequential data in a time-dependent manner (unlike RNNs or LSTMs), we need to explicitly provide information about the position of each word in the input sequence.\n","\n","The Positional Encoding layer is used to add this positional information to the word embeddings. It generates a vector for each position in the sequence and combines it with the word embedding to provide both the content and position information. The positional encoding is computed using sine and cosine functions of different wavelengths, which allows the model to easily learn relative positions.\n","\n","In this step, your task is to implement the Positional Encoding layer. You should:\n","\n","1. Compute the positional encodings using sine and cosine functions.\n","2. Register the positional encodings as a buffer so they are not considered trainable parameters.\n","3. Add the positional encoding to the word embeddings during the forward pass.\n","\n","Once you have implemented the layer, you can test if it works correctly by running the test cell.\n"]},{"cell_type":"code","execution_count":37,"id":"dfbfdac1-3bd3-4a57-b034-4aeb97cf1b9d","metadata":{"id":"dfbfdac1-3bd3-4a57-b034-4aeb97cf1b9d","executionInfo":{"status":"ok","timestamp":1733009650827,"user_tz":-120,"elapsed":5,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, embed_size, max_len=512):\n","        super(PositionalEncoding, self).__init__()\n","        self.embed_size = embed_size\n","\n","        # Initialize a tensor to store positional encodings for each position up to max_len\n","        pos_encoding = torch.zeros(max_len, embed_size)\n","        # Create a tensor for positions, where each position corresponds to a word's position in the sequence\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","\n","        # 1. Create a tensor `div_term` to scale the positional encoding values.\n","\n","        # Hint:\n","        # This is based on the formula for positional encoding where each dimension has a different frequency.\n","        # We generate a range of values from 0 to embed_size, stepping by 2 (for even indices), and multiply it by a scaling factor.\n","        # The scaling factor (-math.log(10000.0) / embed_size) ensures the frequencies decay logarithmically.\n","\n","        # 2. Apply the sine function to the even indices of the positional encoding matrix.\n","        # 3. Apply the cosine function to the odd indices of the positional encoding matrix.\n","\n","        # Hint:\n","        # The `position` tensor holds the position values for each token, and `div_term` scales those values.\n","\n","        # YOUR CODE HERE\n","        div_term = torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size)\n","        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n","        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n","\n","        # Register as buffer so it is not considered as a parameter during training\n","        self.register_buffer('pos_encoding', pos_encoding.unsqueeze(0))\n","\n","    def forward(self, x):\n","        # Add positional encoding to embeddings\n","        x = x * math.sqrt(self.embed_size)\n","        x = x + self.pos_encoding[:, :x.size(1), :].to(x.device)\n","        return x"]},{"cell_type":"code","execution_count":38,"id":"f3a3335e-1dff-4fe8-aebd-8367d2359da5","metadata":{"id":"f3a3335e-1dff-4fe8-aebd-8367d2359da5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009654242,"user_tz":-120,"elapsed":417,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"55ec7eff-bc07-4f1b-e31f-6a7fa0520324"},"outputs":[{"output_type":"stream","name":"stdout","text":["__________\n","torch.Size([1, 10, 128])\n","torch.Size([1, 10, 128])\n"]}],"source":["#%% Applying positional encoding\n","positional_encoding = PositionalEncoding(embedding_size, 512)\n","output_pe_fr = positional_encoding (output_embedding_fr)\n","output_pe_en = positional_encoding (output_embedding_en)\n","print(10*\"_\")\n","print(output_pe_fr.size())\n","print(output_pe_en.size())"]},{"cell_type":"code","execution_count":39,"id":"6fa00a4a-cbc5-4ef4-873f-b4d0dafa33ea","metadata":{"id":"6fa00a4a-cbc5-4ef4-873f-b4d0dafa33ea","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009654861,"user_tz":-120,"elapsed":4,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"93fe51cb-2f99-4943-e7eb-f71d3c32643b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Positional Encoding has been implemented correctly!\n"]}],"source":["# Visible tests here\n","\n","# Test if positional encoding has been implemented correctly\n","\n","# Initialize the positional encoding with a fixed embed size and max_len\n","test_pos_enc = PositionalEncoding(embed_size=128, max_len=512)\n","\n","# Let's assume the input sequence is a batch of embeddings (value doesn't matter for this test)\n","src_batch = torch.zeros(1, 10, 128)  # batch_size=1, seq_len=10, embed_size=128\n","tgt_batch = torch.zeros(1, 10, 128)\n","\n","# Pass through the positional encoding layer\n","encoded_src_batch = test_pos_enc(src_batch)\n","encoded_tgt_batch = test_pos_enc(tgt_batch)\n","\n","# Check the output dimensions\n","assert encoded_src_batch.size() == src_batch.size(), f\"Expected {src_batch.size()}, but got {encoded_src_batch.size()}\"\n","assert encoded_tgt_batch.size() == tgt_batch.size(), f\"Expected {tgt_batch.size()}, but got {encoded_tgt_batch.size()}\"\n","\n","# Check if positional encoding is working correctly\n","\n","# Ensure that adding positional encoding to embeddings changes the values\n","# The output at the first position should be different than at other positions (if positional encoding is applied correctly)\n","assert not torch.allclose(encoded_src_batch[:, 0, :], encoded_src_batch[:, 1, :], atol=1e-5), \\\n","    \"Positional encoding should differentiate between different positions in the sequence.\"\n","\n","print(\"Positional Encoding has been implemented correctly!\")\n","\n"]},{"cell_type":"markdown","id":"6baf7c61-c078-45c3-977e-fbefe51e3beb","metadata":{"id":"6baf7c61-c078-45c3-977e-fbefe51e3beb"},"source":["## Task 2: Model Architecture (5 points)\n","\n","### Summary of Tasks for This Stage\n","\n","**Task 2.1: Designing a basic transformer block** (3 points)\n","\n","**Task 2.2: Adding Encoder and Decoder blocks** (2 points)"]},{"cell_type":"markdown","id":"de1ff3a1-5f60-4baa-9eb9-17d899db9df5","metadata":{"id":"de1ff3a1-5f60-4baa-9eb9-17d899db9df5"},"source":["### Task 2.1: Designing a basic transformer block\n","\n","In this task, you will implement a simple Transformer model. This model will take source and target sequences as input, apply embeddings and positional encodings, pass the result through a Transformer block, and finally project the output to the target vocabulary space. Before passing the input through the Transformer block, you need to compute two types of masks:\n","\n","**Padding Mask:** This mask is applied to the source sequence and to the target sequence to prevent the model from attending to padding tokens, which should be ignored during training. The padding mask is implemented in the create_pad_mask method.\n","\n","**Target Mask (tgt_mask):** This mask is used to prevent the model from using future target steps to predict the current output. If the model could access future information, it would already know the solution, making training redundant. The tgt_mask helps ensure causal attention by masking out future tokens in the target sequence.\n","\n","These two masks are essential for enabling effective training and maintaining the correct flow of information through the model.\n","\n","For this task, you can use the pre-implemented embedding and positional encoding blocks. In the MySimpleTransformer template provided below, fill in the blanks as instructed in the code. Once you have implemented the MySimpleTransformer class, you can test the correctness of your solution by running the test cell.\n","\n","You may receive a warning about using 'batch_first' during the initialization of the Transformer block. Please ignore it!"]},{"cell_type":"code","execution_count":40,"id":"b5f4ccdb-b179-4d23-952d-bdc91600d5b6","metadata":{"id":"b5f4ccdb-b179-4d23-952d-bdc91600d5b6","executionInfo":{"status":"ok","timestamp":1733009657912,"user_tz":-120,"elapsed":394,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["class MySimpleTransformer(nn.Module):\n","    def __init__(self, vocab_size_src, vocab_size_tgt, embed_size, num_heads, hidden_dim, num_encoder_layers, num_decoder_layers, max_len=512):\n","        super(MySimpleTransformer, self).__init__()\n","        # Initialize layers as below:\n","        # Embedding layer for source language tokens\n","        # embedding layer for target langauge tokens\n","        # Positional encoding\n","        # Transformer block (Hint: use nn.Transformer)\n","        # Final linear layer to project transformer output to vocab size (Hint: use nn.Linear)\n","\n","        # YOUR CODE HERE\n","        self.embedding_source = nn.Embedding(vocab_size_src, embed_size)\n","        self.embedding_target = nn.Embedding(vocab_size_tgt, embed_size)\n","        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n","        self.transformer = nn.Transformer(d_model=embed_size,\n","                                          nhead=num_heads,\n","                                          num_encoder_layers=num_encoder_layers,\n","                                          num_decoder_layers=num_decoder_layers,\n","                                          dim_feedforward = hidden_dim)\n","        self.linear = nn.Linear(embed_size, vocab_size_tgt)\n","\n","    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None, tgt_mask=None):\n","\n","        # 1. Get embeddings for source and target.\n","        # 2. apply positional encoding to embedded source and target\n","        # 3. Create a causal mask for the target to prevent seeing future tokens\n","        # 4. Forward pass to Transformer block with masking\n","        # 5. Project to vocabulary size\n","\n","        src = src.transpose(0, 1)\n","        tgt = tgt.transpose(0, 1)\n","        if tgt_mask != None:\n","            tgt_mask = tgt_mask.transpose(0,1)\n","\n","        # YOUR CODE HERE\n","        src_embedded = self.embedding_source(src)\n","        tgt_embedded = self.embedding_target(tgt)\n","        src_positional_encoded = self.positional_encoding(src_embedded)\n","        tgt_positional_encoded = self.positional_encoding(tgt_embedded)\n","        tgt_causal_mask = tgt_mask\n","        output = self.transformer(src_positional_encoded,\n","                                  tgt_positional_encoded,\n","                                  tgt_mask=tgt_causal_mask,\n","                                  src_key_padding_mask=src_padding_mask,\n","                                  tgt_key_padding_mask=tgt_padding_mask)\n","        output = self.linear(output)\n","\n","        output = output.transpose(0, 1)\n","        return output\n","\n","    def get_tgt_mask(self, tgt):\n","        tgt_seq_len = tgt.size(1)\n","        print(tgt.shape)\n","        print(tgt_seq_len)\n","        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n","        return tgt_mask\n","\n","    def create_pad_mask(self, matrix):\n","        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n","        # [False, False, False, True, True, True]\n","        pad_token = 0\n","        return (matrix == pad_token)\n","\n","def get_num_trainable_parameters(model):\n","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f'The model has {num_params} trainable parameters.')\n","    return num_params"]},{"cell_type":"code","execution_count":41,"id":"66a7ffba-b4f2-4235-a602-bb6e6394d3ab","metadata":{"id":"66a7ffba-b4f2-4235-a602-bb6e6394d3ab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009659232,"user_tz":-120,"elapsed":7,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"808d5604-04ee-4888-de56-0ebeaa8e34cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10])\n","10\n","The model has 10641353 trainable parameters.\n","\u001b[92mGood job! All visible tests passed! You can proceed further.\u001b[0m\n"]}],"source":["# Visible tests here\n","all_tests_successful = True\n","\n","embedding_size = 512\n","vsize_src = len(fr_word2idx) # 336\n","vsize_tgt = len(en_word2idx) # 201\n","hdim = 128\n","model = MySimpleTransformer(vsize_src, vsize_tgt, embedding_size, 2, hdim, 3, 3, max_len=512)\n","model = model.to(device)\n","\n","# Data\n","dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","src_batch, tgt_batch = next(iter(dataloader))\n","\n","tgt_mask = model.get_tgt_mask(tgt_batch)\n","src_padding_mask = model.create_pad_mask(src_batch)\n","tgt_padding_mask = model.create_pad_mask(tgt_batch)\n","output = model(src_batch, tgt_batch, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_mask = tgt_mask )\n","\n","# Check if the output shape is correct: [batch_size, seq_len, vocab_size_tgt]\n","if output.size() != (1, 10, vsize_tgt):\n","    all_tests_successful = False\n","    raise AssertionError(f\"Expected output shape (2, 10, {vocab_size_tgt}), but got {output.size()}\")\n","\n","num_params = get_num_trainable_parameters(model)\n","expected_num_parameters = 10641353\n","if num_params != expected_num_parameters:\n","    all_tests_successful = False\n","    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n","\n","\n","if all_tests_successful:\n","    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n","    print(f\"\\033[92m{success_str}\\033[0m\")\n"]},{"cell_type":"code","execution_count":42,"id":"fa97de74-2b5a-4521-9474-04f9b7112e0b","metadata":{"id":"fa97de74-2b5a-4521-9474-04f9b7112e0b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009661464,"user_tz":-120,"elapsed":428,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"e221e7fd-0950-481d-8187-90fb6630b21e"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10])\n","torch.Size([1, 10])\n","torch.Size([1, 10])\n","[[  1  66   5 104  16 115   2   0   0   0]]\n","[[False False False False False False False  True  True  True]]\n","torch.Size([10, 10])\n","[[  0. -inf -inf -inf -inf -inf -inf -inf -inf -inf]\n"," [  0.   0. -inf -inf -inf -inf -inf -inf -inf -inf]\n"," [  0.   0.   0. -inf -inf -inf -inf -inf -inf -inf]\n"," [  0.   0.   0.   0. -inf -inf -inf -inf -inf -inf]\n"," [  0.   0.   0.   0.   0. -inf -inf -inf -inf -inf]\n"," [  0.   0.   0.   0.   0.   0. -inf -inf -inf -inf]\n"," [  0.   0.   0.   0.   0.   0.   0. -inf -inf -inf]\n"," [  0.   0.   0.   0.   0.   0.   0.   0. -inf -inf]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0. -inf]\n"," [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"]}],"source":["\n","print(src_batch.size())\n","print(tgt_batch.size())\n","\n","print(src_padding_mask.size())\n","print(src_batch.cpu().detach().numpy())\n","print(src_padding_mask.cpu().detach().numpy())\n","\n","print(tgt_mask.size())\n","print(tgt_mask.cpu().detach().numpy())"]},{"cell_type":"markdown","id":"ce03f944-f320-4ab2-ba28-cbeb8cb7927a","metadata":{"id":"ce03f944-f320-4ab2-ba28-cbeb8cb7927a"},"source":["### Task 2.2: Adding Encoder and Decoder blocks\n","\n","The MySimpleTransformer class you implemented uses the Transformer module of PyTorch, which consists of two main parts: the encoder and the decoder. Each part of the model contains several layers of self-attention and feedforward neural networks, with the encoder and decoder connected by a cross-attention mechanism.\n","\n","**Self-Attention:** In the context of machine translation, self-attention allows each word in a sequence (either in the source or target language) to attend to every other word in the same sequence, regardless of their position. This mechanism enables the model to capture long-range dependencies and relationships within the sentence.\n","\n","**Cross-Attention:** This occurs in the decoder block, where the model attends to the encoder's output. In machine translation, cross-attention allows the decoder to focus on relevant parts of the input sequence (source language) when generating the output sequence (target language). It essentially \"crosses\" between the encoder and decoder, enabling the model to translate based on the context of both source and target sentences.\n","\n","Next, we will modify our Transformer block by manually separating the encoder and decoder components. This separation is necessary because, during inference (translation), the source sentence must pass through the encoder, and the generated target sentence must pass through the decoder one token at a time. This step is crucial since the translation process is autoregressive, meaning each word is predicted based on the previously generated words.\n","\n","In the cell below, complete the MyTransformer class as an updated version of our model class. This updated version will be used to train our neural machine translation system. Ensure that the encoder and decoder components are implemented separately, as this is important for managing the inference process later.\n","\n","Hints:\n","\n","1. The updated model is similar to the simple model from Task 2.1, except that the encoder and decoder are separated to allow individual calls. Remember to include the source and target padding masks, as well as the target causal mask (tgt_mask).\n","\n","2. The layer initialization for the updated model should be identical to the simple model. Specifically, the *init* method of the MyTransformer class should mirror the MySimpleTransformer class.\n","\n","3. Use nn.Transformer.encoder(src_embedded, src_key_padding_mask=src_padding_mask)\n","\n","4. Use nn.Transformer.decoder(tgt_embedded, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask)\n","\n","Once you have implemented the MyTransformer class, you can test the correctness of your solution by running the test cell.\n"]},{"cell_type":"code","execution_count":43,"id":"a05c4ec1-3f32-4d12-b0e2-a2bd1643b0de","metadata":{"id":"a05c4ec1-3f32-4d12-b0e2-a2bd1643b0de","executionInfo":{"status":"ok","timestamp":1733009663515,"user_tz":-120,"elapsed":3,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["class MyTransformer(nn.Module):\n","    def __init__(self, vocab_size_src, vocab_size_tgt, embed_size, num_heads, hidden_dim, num_encoder_layers, num_decoder_layers, max_len=512):\n","        super(MyTransformer, self).__init__()\n","        # Initialize layers similar to MySimpleTransformer\n","        # Two Embedding layers for source and target text\n","        # Positional encoding\n","        # Transformer block\n","        # Final linear layer to project transformer output to vocab size\n","\n","        # YOUR CODE HERE\n","        self.embedding_source = nn.Embedding(vocab_size_src, embed_size)\n","        self.embedding_target = nn.Embedding(vocab_size_tgt, embed_size)\n","        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n","        self.transformer = nn.Transformer(d_model=embed_size,\n","                                          nhead=num_heads,\n","                                          num_encoder_layers=num_encoder_layers,\n","                                          num_decoder_layers=num_decoder_layers,\n","                                          dim_feedforward = hidden_dim)\n","        self.linear = nn.Linear(embed_size, vocab_size_tgt)\n","\n","    def encode(self, src, src_padding_mask):\n","        # Transpose inputs to (seq_len, batch_size, embedding_dim)\n","        src = src.transpose(0, 1)\n","\n","        # 1. Get embeddings for source\n","        # 2. apply positional encoding to embedded source\n","        # 3. Forward pass to Transformer encoder block with src_key_padding_mask\n","\n","        # YOUR CODE HERE\n","        embedded_src = self.embedding_source(src)\n","        positional_encoded_src = self.positional_encoding(embedded_src)\n","        encoded = self.transformer.encoder(positional_encoded_src,\n","                                           src_key_padding_mask=src_padding_mask)\n","\n","        return encoded\n","\n","    def decode(self, tgt, memory, tgt_mask, tgt_padding_mask):\n","        # Transpose inputs to (seq_len, batch_size, embedding_dim)\n","        tgt = tgt.transpose(0, 1)\n","        if tgt_mask != None:\n","            tgt_mask = tgt_mask.transpose(0,1)\n","        # 1. Get embeddings for target\n","        # 2. apply positional encoding to embedded target\n","        # 3. Forward pass target and memory (output of encode) to Transformer decoder block with tgt_mask\n","\n","        # YOUR CODE HERE\n","        embedded_tgt = self.embedding_target(tgt)\n","        positional_encoded_tgt = self.positional_encoding(embedded_tgt)\n","        decoded = self.transformer.decoder(positional_encoded_tgt,\n","                                           memory, tgt_mask=tgt_mask,\n","                                           tgt_key_padding_mask=tgt_padding_mask)\n","\n","        return decoded\n","\n","    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None, tgt_mask=None):\n","        # 1. pass source through encode block (name it as memory)\n","        # 2. pass target and memory through decode block\n","        # 3. Project to vocabulary size\n","\n","        # YOUR CODE HERE\n","        memory = self.encode(src, src_padding_mask)\n","        output_decoder = self.decode(tgt, memory, tgt_mask, tgt_padding_mask)\n","        output = self.linear(output_decoder)\n","\n","        output = output.transpose(0, 1)\n","        return  output_decoder, output\n","\n","    def get_tgt_mask(self, tgt):\n","        tgt_seq_len = tgt.size(1)\n","        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n","        return tgt_mask\n","\n","    def create_pad_mask(self, matrix):\n","        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n","        # [False, False, False, True, True, True]\n","        pad_token = 0\n","        return (matrix == pad_token)"]},{"cell_type":"code","execution_count":44,"id":"7d421762-6a59-4a20-8c05-594ea71eafac","metadata":{"id":"7d421762-6a59-4a20-8c05-594ea71eafac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733009663897,"user_tz":-120,"elapsed":384,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"c14c9cdf-0c89-4269-e059-57838d90c64a"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 10641353 trainable parameters.\n","\u001b[92mGood job! All visible tests passed! You can proceed further.\u001b[0m\n"]}],"source":["# Visible tests here\n","all_tests_successful = True\n","\n","embedding_size = 512\n","vsize_src = len(fr_word2idx) # 336\n","vsize_tgt = len(en_word2idx) # 201\n","hdim = 128\n","model = MyTransformer(vsize_src, vsize_tgt, embedding_size, 2, hdim, 3, 3, max_len=512)\n","model = model.to(device)\n","\n","# Data\n","dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","src_batch, tgt_batch = next(iter(dataloader))\n","\n","tgt_mask = model.get_tgt_mask(tgt_batch)\n","src_padding_mask = model.create_pad_mask(src_batch)\n","tgt_padding_mask = model.create_pad_mask(tgt_batch)\n","_, output = model(src_batch, tgt_batch, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_mask = tgt_mask )\n","\n","# Check if the output shape is correct: [batch_size, seq_len, vocab_size_tgt]\n","if output.size() != (1, 10, vsize_tgt):\n","    all_tests_successful = False\n","    raise AssertionError(f\"Expected output shape (2, 10, {vocab_size_tgt}), but got {output.size()}\")\n","\n","num_params = get_num_trainable_parameters(model)\n","expected_num_parameters = 10641353\n","if num_params != expected_num_parameters:\n","    all_tests_successful = False\n","    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n","\n","if all_tests_successful:\n","    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n","    print(f\"\\033[92m{success_str}\\033[0m\")"]},{"cell_type":"markdown","id":"f478708b-68d0-4690-8bed-3629ea2e4b2d","metadata":{"id":"f478708b-68d0-4690-8bed-3629ea2e4b2d"},"source":["## Task 3: Training and Evaluation (5 points)\n","\n","So far, we have defined our dataset class and the Transformer model. The next step is to train and validate the model. We will split the data into training and validation sets, with an 80% training and 20% validation ratio, and run the training and validation loops accordingly.\n","\n","For the model, we will define a simple Transformer with a hidden dimension of 512, 4 encoder layers, 4 decoder layers, and 6 attention heads. We will use cross-entropy loss, which is well-suited for Transformer-based machine translation because it measures the difference between the predicted probability distribution and the true distribution for each token in the sequence. This loss function helps the model optimize the prediction accuracy for each word in the target sequence. Additionally, we will use the Adam optimizer to efficiently minimize the loss.\n","\n","First, run the two cells below to define the data and model with the specified hyperparameters."]},{"cell_type":"code","execution_count":45,"id":"5b833b15-348a-447c-9c6e-185ec36736d7","metadata":{"id":"5b833b15-348a-447c-9c6e-185ec36736d7","executionInfo":{"status":"ok","timestamp":1733009665096,"user_tz":-120,"elapsed":4,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["# Data\n","\n","bs = 256\n","dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=7)\n","number_of_sentences = len(tokenized_fr)\n","train_size = int((0.8)*number_of_sentences)\n","test_size = int(number_of_sentences - train_size)\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, drop_last=True)\n","val_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, drop_last=True)"]},{"cell_type":"code","execution_count":46,"id":"efd32c9e-9868-4f8a-819d-6816332a9ea7","metadata":{"id":"efd32c9e-9868-4f8a-819d-6816332a9ea7","executionInfo":{"status":"ok","timestamp":1733009666525,"user_tz":-120,"elapsed":2,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["# Model\n","\n","embedding_size = 240 # embed_dim must be divisible by num_heads\n","vsize_src = len(fr_word2idx) # 336\n","vsize_tgt = len(en_word2idx) # 201\n","hdim = 512\n","model = MyTransformer(vsize_src, vsize_tgt, embedding_size, 6, hdim, 4, 4, max_len=256)\n","model = model.to(device)"]},{"cell_type":"markdown","id":"db1a4a14-51e3-42d2-babd-2fd67708cac0","metadata":{"id":"db1a4a14-51e3-42d2-babd-2fd67708cac0"},"source":["In the cell below, complete the training and validation loops by filling in the blanks as instructed in the code. Once you have implemented the loops, run the cell to train the model for 10 epochs.\n","\n","If you have limited computational resources, you may train for fewer epochs until the validation loss reaches below 0.1 (This can be reached after about 3 epochs). However, keep in mind that the performance of your model will be very poor on the translation task if it is not trained for enough epochs.\n","\n","Once you have completed the training, run the test cell to check if the training and validation loss have decreased to below 0.1. Remember to submit the trained model **model.pth** to Moodle along with your other files."]},{"cell_type":"code","execution_count":48,"id":"17e63f9b-07b4-4d01-a62d-25b9deafaf5a","metadata":{"id":"17e63f9b-07b4-4d01-a62d-25b9deafaf5a","colab":{"base_uri":"https://localhost:8080/","height":929},"executionInfo":{"status":"ok","timestamp":1733010052298,"user_tz":-120,"elapsed":259233,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"ec5e16e5-33b9-4243-e7e1-b974e142b2ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Train Loss: 0.1304\n","Epoch 1/10, Validation Loss: 0.0894\n","Epoch 2/10, Train Loss: 0.0916\n","Epoch 2/10, Validation Loss: 0.078\n","Epoch 3/10, Train Loss: 0.0814\n","Epoch 3/10, Validation Loss: 0.0748\n","Epoch 4/10, Train Loss: 0.077\n","Epoch 4/10, Validation Loss: 0.0726\n","Epoch 5/10, Train Loss: 0.0746\n","Epoch 5/10, Validation Loss: 0.073\n","Epoch 6/10, Train Loss: 0.0733\n","Epoch 6/10, Validation Loss: 0.0713\n","Epoch 7/10, Train Loss: 0.072\n","Epoch 7/10, Validation Loss: 0.0696\n","Epoch 8/10, Train Loss: 0.0711\n","Epoch 8/10, Validation Loss: 0.0696\n","Epoch 9/10, Train Loss: 0.0704\n","Epoch 9/10, Validation Loss: 0.0699\n","Epoch 10/10, Train Loss: 0.0699\n","Epoch 10/10, Validation Loss: 0.0687\n","Training completed.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Training\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","num_epochs = 10\n","\n","if not skip_training:\n","    epoch_train_losses = []\n","    epoch_validation_losses = []\n","    for epoch in range(num_epochs):\n","        model = model.to(device)\n","        model.train()  # Set model to training mode\n","        total_loss = 0\n","        num_samples = 0\n","        for src_batch, tgt_batch in train_loader:\n","            optimizer.zero_grad()\n","            # set input source as src_batch\n","            # set input target as start of target sentence untill -1 (i.e. discard the last token) (past) [name it as tgt_input]\n","            # set expexted target as target sentence from 1 to the end (i.e. discard the first token) (future) [name it as tgt_expected]\n","            # use input source and input target (tgt_input) as input to the model\n","            # expexted target will be used in loss to compare it with the model output.\n","            # Remember to use padding and target causal masks on the model call:\n","            # get padding mask of source,\n","            # get padding mask of input target, convert it to float (.float()),\n","            # get tgt_mask of input target,\n","            # pass inout source, input target, source padding mask, and tgt_mask to the model to get the predictions (output).\n","\n","            # YOUR CODE HERE\n","            src_batch = src_batch.to(device)\n","            tgt_batch = tgt_batch.to(device)\n","            src_input = src_batch\n","            tgt_input = tgt_batch[:, :-1]\n","            tgt_expected = tgt_batch[:, 1:]\n","            src_padding_mask = model.create_pad_mask(src_input).to(device)\n","            tgt_padding_mask = model.create_pad_mask(tgt_input).float().to(device)\n","            tgt_mask = model.get_tgt_mask(tgt_input).to(device)\n","            _, output = model(src_input, tgt_input, src_padding_mask, tgt_padding_mask, tgt_mask)\n","\n","\n","            output = output.to(device)\n","            output = output.contiguous().view(-1, vsize_tgt)\n","            tgt_expected = tgt_expected.contiguous().view(-1)\n","\n","            loss = criterion(output, tgt_expected)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            num_samples += src_batch.shape[0]\n","\n","        epoch_train_loss = total_loss / len(train_loader)\n","        epoch_train_loss = round(epoch_train_loss, 4)\n","        epoch_train_losses.append(epoch_train_loss)\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_train_loss}\")\n","\n","        ################################################################\n","\n","        model.eval()\n","        validation_loss = 0\n","        num_samples = 0\n","        with torch.no_grad():\n","            for src_batch, tgt_batch in val_loader:\n","                # set input source as src_batch\n","                # set input target as start of target sentence untill -1 (i.e. discard the last token) (past) [name it as tgt_input]\n","                # set expexted target as target sentence from 1 to the end (i.e. discard the first token) (future) [name it as tgt_expected]\n","                # use input source and input target (tgt_input) as input to the model\n","                # expexted target will be used in loss to compare it with the model output.\n","                # Remember to use padding and target causal masks on the model call:\n","                # get padding mask of source,\n","                # get padding mask of input target, convert it to float (.float()),\n","                # get tgt_mask of input target,\n","                # pass inout source, input target, source padding mask, and tgt_mask to the model to get the predictions (output).\n","\n","                # YOUR CODE HERE\n","                src_batch = src_batch.to(device)\n","                tgt_batch = tgt_batch.to(device)\n","                src_input = src_batch\n","                tgt_input = tgt_batch[:, :-1]\n","                tgt_expected = tgt_batch[:, 1:]\n","                src_padding_mask = model.create_pad_mask(src_input).to(device)\n","                tgt_padding_mask = model.create_pad_mask(tgt_input).float().to(device)\n","                tgt_mask = model.get_tgt_mask(tgt_input).to(device)\n","                _, output = model(src_input, tgt_input, src_padding_mask, tgt_padding_mask, tgt_mask)\n","\n","                output = output.to(device)\n","                output = output.contiguous().view(-1, vsize_tgt)\n","                tgt_expected = tgt_expected.contiguous().view(-1)\n","                loss = criterion(output, tgt_expected)\n","                validation_loss += loss.item()\n","                num_samples += src_batch.shape[0]\n","\n","            epoch_validation_loss = validation_loss / len(val_loader)\n","            epoch_validation_loss = round(epoch_validation_loss, 4)\n","            epoch_validation_losses.append(epoch_validation_loss)\n","\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {epoch_validation_loss}\")\n","        torch.save(model.state_dict(), 'model.pth')\n","\n","    print(\"Training completed.\")\n","    torch.save(model.state_dict(), 'model.pth')\n","\n","    plt.figure(figsize=(10, 6))\n","    epochs = range(1, num_epochs + 1)\n","    plt.plot(epochs, epoch_train_losses, label='Train Loss', color='blue', marker='o')\n","    plt.plot(epochs, epoch_validation_losses, label='Validation Loss', color='red', marker='x')\n","    plt.title('Train vs Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()"]},{"cell_type":"code","execution_count":49,"id":"463af138-b291-49b6-a74c-521328e684a6","metadata":{"id":"463af138-b291-49b6-a74c-521328e684a6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733010052298,"user_tz":-120,"elapsed":11,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"57f01ae1-026b-4857-a16b-33abaa66843e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92mGood job! The final train loss and validation loss are in the expected range!\u001b[0m\n"]}],"source":["# Visible tests here\n","all_tests_successful = True\n","# Visible tests for checking the performance of the trained model\n","# Test if the train and validation losses are within the correct range\n","if not skip_training:\n","    if not (epoch_train_loss <= 0.1):\n","        all_tests_successful = False\n","        raise AssertionError(f\"Training loss {epoch_train_loss} must be smaller than 0.1\")\n","\n","    if not (epoch_validation_loss <= 0.1):\n","        all_tests_successful = False\n","        raise AssertionError(f\"Validation loss {epoch_validation_loss} must be smaller than 0.1\")\n","\n","    if all_tests_successful:\n","        success_str = \"Good job! The final train loss and validation loss are in the expected range!\"\n","        print(f\"\\033[92m{success_str}\\033[0m\")"]},{"cell_type":"code","execution_count":50,"id":"72146f6b-9f87-4d59-9833-d62c12257eba","metadata":{"id":"72146f6b-9f87-4d59-9833-d62c12257eba","executionInfo":{"status":"ok","timestamp":1733010052298,"user_tz":-120,"elapsed":8,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["# Do not delete this cell"]},{"cell_type":"markdown","id":"e4d97d50-f344-4bd0-9933-31e0967db8ff","metadata":{"id":"e4d97d50-f344-4bd0-9933-31e0967db8ff"},"source":["## Task 4: Autoregressive Translation (5 points)\n","\n","Finally, we are going to use the trained model to perform an actual translation task, translating French sentences into English. Exciting!\n","\n","The inference works in an autoregressive manner. This means that, during the translation process, the model generates each token in the target sequence one at a time, using the previously generated token as input for predicting the next one. At each step, the model uses the encoded source sentence along with the target sequence generated so far to predict the next word. This approach allows the model to produce the translation step by step, instead of generating the entire sequence at once.\n","\n","The steps for translation are as follows:\n","\n","**Source Sentence Encoding:** First, we obtain the source sequence embedding and pass it through the encoder to obtain its encoded representation.\n","\n","**Initializing Target Sentence with <SOS> Token:** We initialize the target sentence with the special <SOS> (Start Of Sentence) token, which indicates the beginning of the translation.\n","\n","**Autoregressive Loop to Translate Target Tokens One at a Time:** We enter a loop where the model predicts the next token in the sequence based on the previously generated token and the encoded source sentence. This loop continues until the model predicts the <EOS> (End Of Sentence) token or the maximum sequence length is reached.\n","\n","In the cell below, fill in the blanks as instructed to create the translation loop for the provided example sentences. Once you have completed the template, run it and observe the printed translation results.\n","\n","Remeber to submit **'translation.npy'** to Moodle along with your other files."]},{"cell_type":"code","execution_count":67,"id":"7b511cb3-8a3d-4d83-b403-ffb9f4b65f5e","metadata":{"id":"7b511cb3-8a3d-4d83-b403-ffb9f4b65f5e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733012348852,"user_tz":-120,"elapsed":518,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}},"outputId":"8ef4ee95-aa43-4f20-cc7a-df237f68682d"},"outputs":[{"output_type":"stream","name":"stdout","text":["original_sentence: new jersey est parfois calme pendant l' automne.\n","translated_sentence: during autumn <EOS>\n","----------\n","original_sentence: california est généralement calme en mars.\n","translated_sentence: during march <EOS>\n","----------\n"]}],"source":["seq_len=10\n","start_token=1\n","end_token=2\n","model.eval()\n","\n","# Convert src_sentence to tokenized integers in the vocabulary dictionary\n","example_source_sentences = [\"new jersey est parfois calme pendant l' automne.\", \"california est généralement calme en mars.\"]\n","example_tokenized = tokenize(example_source_sentences)\n","src_sentences = []\n","for ex in example_tokenized:\n","    ex_inds = []\n","    for t in ex:\n","        t_ind = fr_word2idx [t]\n","        ex_inds.append(t_ind)\n","    src_sentences.append(ex_inds)\n","\n","translated_sequences = []\n","for counter, src_sentence in enumerate(src_sentences):\n","    # Convert source tokens to Tensor\n","    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)  # Shape: (1, src_seq_length)\n","\n","    # 1. create src_padding_mask\n","    # 2. get \"memory\" by passing source with create src_padding_mask through encode block (model.encode)\n","\n","    # YOUR CODE HERE\n","    src_padding_mask = model.create_pad_mask(src_tensor).to(device)\n","    memory = model.encode(src_tensor, src_padding_mask)\n","\n","    # initialize the predicted tgt_tokens (translation) with start token\n","    tgt_tokens = torch.ones(1, 1).fill_(start_token).type(torch.long).to(device) #(1,1)\n","\n","    for i in range(seq_len-1):\n","        # 1. Mask out the unpredicted tokens in the target (i.e., get tgt_mask)\n","        # 2. get output by passing target (the generated part up to current `i`) and memory to decode block (model.decode)\n","        # 3. remember to pass also tgt_mask\n","        # 4. get a probability vector by passing output through linear layer (projection to vocabulary size)\n","        # 5. use \"torch.max\" to get the index of the predicted word\n","        # 6. Convert it to a tensor on device (name it as \"next_tgt_item\")\n","        # 7. add \"next_tgt_item\" to tgt_tokens (use torch.cat)\n","        # 8. Stop (break) if \"end_token\" is generated\n","\n","        # YOUR CODE HERE\n","        tgt_mask = model.get_tgt_mask(tgt_tokens)\n","        tgt_padding_mask = model.create_pad_mask(tgt_tokens).float().to(device)\n","        output = model.decode(tgt_tokens, memory, tgt_mask, tgt_padding_mask)\n","        output = model.linear(output)\n","        _, next_tgt_item = torch.max(output[-1, 0, :], -1)\n","        next_tgt_item = torch.tensor(next_tgt_item, dtype=torch.long).unsqueeze(0).unsqueeze(0).to(device)\n","        tgt_tokens = torch.cat([tgt_tokens, next_tgt_item], axis=1)\n","        if next_tgt_item == end_token:\n","            break\n","\n","    translated_tokens = tgt_tokens.squeeze().tolist()\n","    translated_sentence = ' '.join ([en_idx2word[i] for i in translated_tokens[1:]])\n","    translated_sequences.append(translated_tokens)\n","    print(\"original_sentence:\", example_source_sentences[counter])\n","    print(\"translated_sentence:\", translated_sentence)\n","    print(10*'-')\n","\n","np.save('translation.npy', np.array(translated_sequences, dtype=object))"]},{"cell_type":"code","execution_count":58,"id":"cce7f314-8db1-408a-bf5f-e91daf531029","metadata":{"id":"cce7f314-8db1-408a-bf5f-e91daf531029","executionInfo":{"status":"ok","timestamp":1733011726427,"user_tz":-120,"elapsed":1,"user":{"displayName":"Thịnh Kiều","userId":"05356630795285410823"}}},"outputs":[],"source":["# Do not delete this cell"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"toc":{"base_numbering":0},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}